\raghav{TODO: learn a word that's not ``approach''}
A naive approach to vectorizing arbitrary arithmetic circuits looks a lot like SLP: We start by serializing the circuit into a sequence of scalar three-address code.
At each step, we look at all available scalar instructions (i.e. instructions whose operands have all already been scheduled), pick the largest set with the same operation, and schedule them together.
The problem with this naive approach is it makes no guarantees about values being computed and used on the same lane; in other words, it incurs arbitrarily many shuffles to make the computation line up. 
Unlike in normal vectorization, where applying arbitrary permutations to the lanes is relatively cheap, in FHE we are only allowed to rotate the entire vector by a fixed number of slots, and this rotation operation is expensive.
Hence, the cost of bookkeeping quickly outweighs whatever benefit we might get from vectorization, making this approach not worth it.

The fundamental problem that makes vectorization in an FHE setting very hard is the high cost of moving data between vector lanes. 
Data movement is actually impossible to avoid: in an arithmetic circuit that computes a single expression, all the intermediate nodes eventually feed into the root, meaning that without data movement every instruction must be on the same line; in other words, no vectorization can take place.

Here we have two extremes: on one end of the spectrum is the SLP approach, which packs aggressively without considering data movement; the other end avoids data movement entirely, precluding any vectorization within individual expressions.
The key idea of our approach is to balance between these two extremes, by finding highly vectorizable subexpressions and evaluating them together with each one on its own lane.

\subsection{Selecting Vectorizable Subexpressions} \raghav{I need annotations on this entire subsection of which things don't make sense and need to be rewritten, because I am certain there are several.}
\subsubsection*{Measuring structural similarity between subtrees}
The structural similarity of two expression trees is used as a proxy for their vectorizability; in particular, we want to measure how well the instruction sequences corresponding to two arbitrary expression trees line up.
Given an expression tree, we can inductively compute the structural similarity between every pair of independent subtrees (two trees are independent if neither root is an ancestor of the other) as follows:
The base case is comparing any tree to a leaf node (a single variable with no operations), in which case the similarity is $0$, as there are no operations that can be packed together.

In the inductive case of two nontrivial trees, there are six ways they could be lined up:
Either root could be lined up with either child of the other tree in four ways. 
Or, the two roots could line up, giving two ways for their children to line up (left with left and right with right, or left with right and right with left).
In the first four cases, the similarity score of the two trees is the similarity score of the alignment. 
In the last two cases, the similarity score of the two trees is the sum of the scores of the aligned children, plus a 1 if the roots have the same operation.
The final similarity score of the two trees is the maximum possible score out of all six cases. 
\raghav{This makes no sense even to me, and I was looking at the code when I wrote this.}
\subsubsection*{Choosing maximally vectorizable sets}
Not every pair of subexpressions are eligible to be vectorized together, since they have to be independent of each other.
This data can be encoded in an undirected {\em vectorizability graph}, where each subexpression is a vertex, and there is an edge between two vertices if the corresponding subexpressions are independent (and thus vectorizable).
Cliques in this graph correspond exactly to sets of expressions that can all be vectorized together. 
We can further label each edge with the similarity score of the two subexpressions it connects.
Since each similarity score roughly corresponds to the number of instructions that could be packed together, the total weight of a clique represents the total number of operations we save by vectorizing together all the subexpressions contained within. 
By simply subtracting a fixed amount from the weight of each edge we can penalize large cliques which would incur a much higher rotation overhead, unless the vectorizability of the clique is high enough to be worth it.
The problem of finding a set of subexpressions to vectorize together reduces to finding a maximal weight clique in this graph, which is easily packaged off to an SMT solver.

% Talk about repeating this process (i.e. cutting out the clique, updating weights to account for this, and repeating the process)
A single round of this technique greedily selects the optimal ``breakpoints'' up to which to vectorize before inserting rotations, so we need to iterate until the entire program has been scheduled.
Once a set of breakpoints is selected \raghav{Yeah, this is the first time I'm officially using the word breakpoint so far, I should introduce it earlier. Terminology is {\em hard}.}, they need to be ``quotiented out'' (i.e. each subexpression needs to be replaced by a single leaf node in the expression tree).
This has two effects on the vectorizability graph: First, all the nodes that appear in the quotient need to be eliminated (including the {\em sub}-subexpressions of the chosen subexpressions), since they are no longer eligible to be vectorized.
However, removing these subexpressions also affects the similarities of their ancestors, since they now have fewer operations that can be packed together.
These changes must be reflected by updating the weights of the vectorizability graph.
To avoid having to recompute all the similarities each time, we store for each edge the list of operations it could pack together; the edge weight can be recovered as the sum of the costs of all of these (in the simplest case, the length).
Each time a node is quotiented out, it is removed from each list that contains it; doing so automatically updates all the edge weights to account for its removal.
Now, we can repeat the above process of finding a maximal clique and vectorizing it, until eventually all nontrivial cliques have a negative total weight, meaning that there are no more subexpressions that are vectorizable enough to make the rotation costs worth it, so we just emit scalar code for the rest of the program.

\subsection{Lane Placement}
\subsection{Code Generation}
